# LLM Evaluation Framework

Фреймворк для оценки языковых моделей с поддержкой базовой и расширенной оценки.

## Возможности

### Базовая оценка (рекомендуется для большинства случаев)
- Измерение скорости генерации
- Мониторинг системных ресурсов (CPU, RAM, GPU)
- Быстрая проверка производительности
- 10 основных метрик за несколько секунд

### Расширенная оценка (для профессионального использования)
- Все возможности базовой оценки
- Тесты точности с помощью LM Evaluation Harness
- Полная оценка качества модели
- Детальный анализ по различным задачам

### Общие возможности
- Работа только с предзагруженными моделями
- Сохранение результатов в JSON формате
- Гибкая настройка параметров оценки
- Поддержка сравнения нескольких моделей

## Установка зависимостей

```bash
pip install transformers torch lm-eval psutil GPUtil
```

## Использование

### Из Jupyter Notebook

#### 1. Базовая оценка (рекомендуется для большинства случаев)

```python
# Добавляем путь к модулю в системный путь Python
import sys
sys.path.append('./src')

# Импортируем необходимые компоненты
from transformers import AutoTokenizer, AutoModelForCausalLM
from main import evaluate_basic_model

# Загрузка модели и токенизатора
model_name = "./Текстовые/Qwen3-0.6B"  # Путь к вашей модели
tokenizer = AutoTokenizer.from_pretrained(model_name)  # Токенизатор для преобразования текста в токены
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",  # Автоматический выбор типа данных (float16/float32)
    device_map="auto"    # Автоматическое размещение на CPU/GPU
)

# Базовая оценка модели
results = evaluate_basic_model(
    model=model,                    # Предзагруженная модель (ОБЯЗАТЕЛЬНО)
    tokenizer=tokenizer,            # Предзагруженный токенизатор (ОБЯЗАТЕЛЬНО)
    model_name="Qwen3-0.6B",        # Название модели для логирования
    num_samples=10,                 # Количество образцов для измерения скорости
    save_results=True               # Сохранять ли результаты в JSON файл
)
```

#### 2. Расширенная оценка (включает тесты точности)

```python
# Импортируем функцию расширенной оценки
from main import evaluate_full_model

# Расширенная оценка с тестами точности
results = evaluate_full_model(
    model=model,                    # Предзагруженная модель (ОБЯЗАТЕЛЬНО)
    tokenizer=tokenizer,            # Предзагруженный токенизатор (ОБЯЗАТЕЛЬНО)
    model_name="Qwen3-0.6B",        # Название модели для логирования
    tasks=["hellaswag", "gsm8k"],   # Список задач для оценки точности
    batch_size=4,                   # Размер батча (влияет на скорость и потребление памяти)
    num_samples=10,                 # Количество образцов для измерения скорости
    save_results=True               # Сохранять ли результаты в JSON файл
)
```

#### 3. Расширенный способ с детальным контролем (для продвинутых пользователей)

```python
from main import ModelEvaluator

# Создание оценщика с детальным контролем
evaluator = ModelEvaluator(
    model=model,                    # Предзагруженная модель
    tokenizer=tokenizer,            # Предзагруженный токенизатор
    model_name="Qwen3-0.6B"         # Название для логирования
)

# Этап 1: Базовая оценка
basic_results = evaluator.run_basic_evaluation(
    num_samples=10,                 # Количество образцов для измерения скорости
    save_results=False              # Не сохранять в файл
)

# Этап 2: Измерение скорости генерации
speed_metrics = evaluator.measure_generation_speed(
    num_samples=10                  # Количество тестовых промптов для измерения скорости
)

# Этап 3: Расширенная оценка с настройками
full_results = evaluator.run_full_evaluation(
    tasks=["hellaswag", "mmlu", "gsm8k"],  # Список задач для оценки точности
    batch_size=8,                          # Размер батча для обработки задач
    num_samples=20,                        # Количество образцов для измерения скорости
    save_results=True                      # Сохранить результаты в файл
)
```

## Подробное описание параметров

### evaluate_basic_model() - Базовая оценка

#### Обязательные параметры:
- **`model`**: Предзагруженная модель (AutoModelForCausalLM)
  - Должна быть загружена заранее через `AutoModelForCausalLM.from_pretrained()`
  - Определяет архитектуру и веса модели для оценки

- **`tokenizer`**: Предзагруженный токенизатор (AutoTokenizer)
  - Должен быть загружен заранее через `AutoTokenizer.from_pretrained()`
  - Преобразует текст в токены для модели и обратно

#### Опциональные параметры:
- **`model_name`** (str, optional): Название модели
  - Используется для логирования и именования файлов результатов
  - Если не указано, используется "preloaded_model"
  - Пример: "Qwen3-0.6B", "LLaMA-7B", "GPT-2"

- **`num_samples`** (int): Количество образцов для измерения скорости
  - По умолчанию: 10
  - Меньшие значения (5-10): быстрая оценка
  - Большие значения (20-50): точная оценка

- **`save_results`** (bool): Сохранение результатов в файл
  - По умолчанию: True
  - True: сохранить в JSON файл с временной меткой
  - False: только в памяти, не сохранять

### evaluate_full_model() - Расширенная оценка

#### Обязательные параметры:
- **`model`**: Предзагруженная модель (AutoModelForCausalLM)
- **`tokenizer`**: Предзагруженный токенизатор (AutoTokenizer)

#### Опциональные параметры:
- **`model_name`** (str, optional): Название модели
- **`tasks`** (list): Список задач для оценки точности
  - По умолчанию: `["hellaswag", "mmlu", "gsm8k"]`
  - Доступные задачи: hellaswag, mmlu, gsm8k, arc_easy, arc_challenge, truthfulqa, winogrande, piqa

- **`batch_size`** (int): Размер батча для обработки
  - По умолчанию: 8
  - Меньшие значения (1-4): экономия памяти, медленная работа
  - Большие значения (8-16): быстрая работа, больше памяти
  - Рекомендуется: 4-8 для большинства случаев

- **`num_samples`** (int): Количество образцов для измерения скорости
  - По умолчанию: 10
  - Меньшие значения (5-10): быстрая оценка
  - Большие значения (20-50): точная оценка

- **`save_results`** (bool): Сохранение результатов в файл
  - По умолчанию: True
  - True: сохранить в JSON файл с временной меткой
  - False: только в памяти, не сохранять

### ModelEvaluator - Расширенный контроль (для продвинутых пользователей)

#### Методы класса:
- **`run_basic_evaluation()`**: Базовая оценка через класс
- **`run_full_evaluation()`**: Расширенная оценка через класс
- **`measure_generation_speed()`**: Только измерение скорости генерации

#### Дополнительные возможности:
- Более детальное логирование
- Доступ к промежуточным результатам
- Возможность поэтапного выполнения

## Доступные задачи оценки (только для расширенной оценки)

### Основные задачи:
- **`hellaswag`**: HellaSwag - здравый смысл и логика
- **`mmlu`**: Massive Multitask Language Understanding - многозадачное понимание языка
- **`gsm8k`**: Grade School Math 8K - математические задачи

### Дополнительные задачи:
- **`arc_easy`**: AI2 Reasoning Challenge (Easy) - рассуждения, легкий уровень
- **`arc_challenge`**: AI2 Reasoning Challenge (Challenge) - рассуждения, сложный уровень
- **`truthfulqa`**: TruthfulQA - правдивость и точность ответов
- **`winogrande`**: Winogrande - разрешение местоимений и контекста
- **`piqa`**: Physical IQa - физический здравый смысл

## Базовые метрики (измеряются всегда)

### Временные метрики:
1. **Время загрузки**: 0.0 сек (модель предзагружена)
2. **Время оценки**: 0.0 сек (базовая) / N сек (расширенная)
3. **Время генерации**: секунды
4. **Общее время**: сумма всех времен

### Системные ресурсы:
5. **Использование RAM**: текущее/общее GB (процент)
6. **Использование CPU**: средний процент (количество ядер)
7. **Использование GPU VRAM**: текущее/общее GB
8. **Загрузка GPU**: процент использования

### Производительность:
9. **Скорость генерации**: токенов/сек
10. **Обработано токенов**: общее количество

## Структура результатов

### Базовая оценка

Функции базовой оценки возвращают словарь с основными метриками:

```python
{
    # Основная информация о модели
    "model_name": "Qwen3-0.6B",                    # Название модели
    
    # Временные метрики
    "load_time": 0.0,                              # Время загрузки (всегда 0 для предзагруженных)
    "evaluation_time": 0.0,                        # Время оценки (0 для базовой оценки)
    "generation_time": 45.2,                       # Время измерения скорости генерации
    "total_time": 45.2,                            # Общее время (равно времени генерации)
    
    # Метрики производительности
    "generation_speed": 15.2,                      # Средняя скорость генерации (токенов/сек)
    "total_tokens_generated": 1000,                # Общее количество сгенерированных токенов
    
    # Системная информация
    "system_metrics": {
        "initial": {...},                          # Системные ресурсы в начале
        "final": {...},                            # Системные ресурсы в конце
        "model_load_time": 0.0,                    # Время загрузки модели
        "evaluation_time": 0.0,                    # Время оценки (0 для базовой)
        "generation_time": 45.2                    # Время измерения скорости
    },
    
    # Файлы и результаты
    "results_file": "Qwen3-0.6B_basic_evaluation_20241201_143022.json",  # Путь к сохраненному файлу
    "generation_speed_detailed": {                 # Детальная статистика генерации
        "average_tokens_per_second": 15.2,
        "total_tokens": 1000,
        "total_time": 45.2,
        "detailed_stats": [...]                    # Статистика по каждому образцу
    }
}
```

### Расширенная оценка

Функции расширенной оценки возвращают словарь с полными результатами:

```python
{
    # Основная информация о модели
    "model_name": "Qwen3-0.6B",                    # Название модели
    
    # Временные метрики
    "load_time": 0.0,                              # Время загрузки (всегда 0 для предзагруженных)
    "evaluation_time": 120.5,                      # Время выполнения оценки точности (секунды)
    "generation_time": 45.2,                       # Время измерения скорости генерации
    "total_time": 165.7,                           # Общее время (оценка + генерация)
    
    # Метрики производительности
    "generation_speed": 15.2,                      # Средняя скорость генерации (токенов/сек)
    "total_tokens_generated": 1000,                # Общее количество сгенерированных токенов
    
    # Системная информация
    "system_metrics": {
        "initial": {...},                          # Системные ресурсы в начале
        "final": {...},                            # Системные ресурсы в конце
        "model_load_time": 0.0,                    # Время загрузки модели
        "evaluation_time": 120.5,                  # Время оценки точности
        "generation_time": 45.2                    # Время измерения скорости
    },
    
    # Файлы и результаты
    "results_file": "Qwen3-0.6B_evaluation_20241201_143022.json",  # Путь к сохраненному файлу
    "lm_eval_results": {...},                      # Детальные результаты LM Evaluation Harness
    "generation_speed_detailed": {                 # Детальная статистика генерации
        "average_tokens_per_second": 15.2,
        "total_tokens": 1000,
        "total_time": 45.2,
        "detailed_stats": [...]                    # Статистика по каждому образцу
    }
}
```

### Детализация system_metrics:
```python
"system_metrics": {
    "initial": {
        "cpu": {"cpu_avg_percent": 25.5, "cpu_count_logical": 16},
        "memory": {"used_gb": 8.2, "total_gb": 32.0, "percent": 25.6},
        "gpu": {"memory_used_gb": 2.1, "memory_total_gb": 24.0, "utilization_percent": 15.2}
    },
    "final": {
        "cpu": {"cpu_avg_percent": 45.2, "cpu_count_logical": 16},
        "memory": {"used_gb": 12.8, "total_gb": 32.0, "percent": 40.0},
        "gpu": {"memory_used_gb": 18.5, "memory_total_gb": 24.0, "utilization_percent": 85.3}
    }
}
```

### Детализация lm_eval_results:
```python
"lm_eval_results": {
    "results": {
        "hellaswag": {
            "acc,none": 0.7523,                    # Точность на задаче
            "acc_stderr,none": 0.0041              # Стандартная ошибка
        },
        "gsm8k": {
            "exact_match,none": 0.2341,            # Точное совпадение
            "exact_match_stderr,none": 0.0089      # Стандартная ошибка
        }
    }
}
```

## Системные требования

### Минимальные требования:
- **Python**: 3.7+
- **RAM**: 4GB (для базовой оценки), 8GB (для расширенной оценки)
- **GPU**: CUDA-совместимая (рекомендуется)

### Рекомендуемые требования:
- **RAM**: 16GB+ (для больших моделей)
- **GPU**: NVIDIA с 8GB+ VRAM
- **CPU**: 8+ ядер

### Зависимости:
- torch
- transformers
- lm-eval (только для расширенной оценки)
- psutil
- GPUtil
- numpy
- tqdm

### Время выполнения:
- **Базовая оценка**: 10-30 секунд
- **Расширенная оценка**: 5-30 минут (зависит от задач и размера модели)

### Детализация зависимостей:
- `torch`: для вычислений и работы с моделями
- `transformers`: для работы с моделями Hugging Face
- `lm-eval`: для оценки точности (только расширенная оценка)
- `psutil`: для мониторинга системных ресурсов
- `GPUtil`: для мониторинга GPU
- `numpy`: для численных вычислений
- `tqdm`: для отображения прогресса

## Логирование

### Файлы логов:
- **`model_evaluation.log`**: Основной файл логов
- **`{model_name}_evaluation_results_{timestamp}.json`**: Результаты оценки

### Уровни логирования:
- **INFO**: Основная информация о процессе
- **WARNING**: Предупреждения о ресурсах
- **ERROR**: Ошибки выполнения

## Поддерживаемые модели

Фреймворк поддерживает все модели, совместимые с Hugging Face Transformers:

### Популярные семейства:
- **Qwen**: Qwen1.5, Qwen2, Qwen3
- **LLaMA**: LLaMA-2, LLaMA-3, Code Llama
- **GPT**: GPT-2, GPT-3, GPT-4
- **BLOOM**: BLOOM, BLOOMZ
- **T5**: T5, Flan-T5, mT5

### Требования к моделям:
- Должны быть в формате Hugging Face
- Поддерживать генерацию текста
- Иметь соответствующий токенизатор

## Важные замечания

### Обязательные требования:
- **Модель и токенизатор должны быть загружены заранее** - модуль не поддерживает загрузку моделей по имени
- **Обязательные параметры**: `model` и `tokenizer` должны быть переданы при инициализации

### Рекомендации по использованию:
- **Эффективность**: Модуль оптимизирован для работы с уже загруженными моделями
- **Гибкость**: Можно выполнять как полную оценку, так и отдельные этапы
- **Память**: Следите за использованием GPU памяти при больших моделях
- **Время**: Полная оценка может занять от нескольких минут до часов

### Типичные сценарии использования:

#### Базовая оценка (быстрая):
- **Быстрая проверка**: `evaluate_basic_model(num_samples=10)`
- **Точное измерение скорости**: `evaluate_basic_model(num_samples=50)`
- **Сравнение моделей**: `evaluate_basic_model()` для нескольких моделей

#### Расширенная оценка (полная):
- **Быстрая оценка точности**: `evaluate_full_model(tasks=["hellaswag"], num_samples=10)`
- **Точная оценка**: `evaluate_full_model(tasks=["hellaswag", "mmlu", "gsm8k"], num_samples=50)`
- **Экономия памяти**: `evaluate_full_model(batch_size=2, num_samples=5)`
